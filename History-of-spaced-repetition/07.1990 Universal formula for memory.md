# 1990: Universal formula for memory

# 1990:记忆的通用公式

[TOC=2,5]

## Optimum review vs. intermittent review

## 最佳回顾与间歇回顾

By 1990, I had no doubt. I had a major discovery at hands. I cracked the problem of [forgetting](https://supermemo.guru/wiki/Forgetting). I knew the [optimum timing](https://supermemo.guru/wiki/Spaced_repetition) of review for simple memories. Once I secured the permission to describe my findings in my [Master's Thesis](https://supermemo.guru/wiki/Optimization_of_learning), my appetite for discovery kept growing. I hoped I might find a universal formula for long-term memory. A formula that would help me track the behavior of memory for any pattern of exposure or retrieval.

到 1990 年，我已经毫无疑问了。我手头有一项重大发现。我解决了遗忘的问题。我知道了简单记忆复习的[最佳时间](https://supermemo.guru/wiki/Spaced_repetition)。当我在硕士论文中获得描述我的发现的许可后，我去探索的欲望就一直在增长。我希望能找到一个长期记忆的通用公式。这个公式可以帮助我跟踪任何暴露或检索模式下的记忆行为。

I already had a collection of data that might help me find the formula. Before discovering the optimum spacing of repetitions in [1985](https://supermemo.guru/wiki/Birth_of_SuperMemo), I used pages of questions for review of knowledge. The review was chaotic and determined by the availability of time, the need, or the mood. I called that "*intermittent learning*". I had recall data for individual pages and for each review. That was the ideal kind of data that did not have the periodicity of [SuperMemo](https://supermemo.guru/wiki/SuperMemo). The exact kind of data needed to solve the problem of memory. However, I had that data on paper only.

我已经收集了一些数据来帮助我找到公式。在 1985 发现重复的最佳间隔之前，我使用问题页来复习知识。这种复习是混乱的，而且取决于时间的利用率，需求，或心情。我称之为“间歇学习”。我有每个页面和每个评论的回顾数据。这是一种理想的数据类型，它不具有 SuperMemo 的周期性。它正是解决记忆问题需要的确切类型的数据。然而，我只有纸上的数据。

In Spring 1990, I recruited my sister to do the typing. No. I do not have a younger sister who would do that eagerly. My sister was 17 years my senior. Being a bit inconsiderate for her time, I used her love to make her do the donkey work. I feel guilty about it. She died just two years later. I never had a chance to repay her contribution to the theory of [spaced repetition](https://supermemo.guru/wiki/Spaced_repetition), which she never even had a chance to understand. Starting on May 1, 1990, she used my time away from the PC to transfer the data from paper to the computer. It took her many days of slow typing. It was worth it.

1990 年春，我请妹妹来打字。不，我没有一个妹妹愿意这样做。我姐姐比我大 17 岁。我有点不体谅她，用她的爱让她干这些打字活。我为此感到内疚。两年后她去世了。我从来没有机会报答她对间隔重复理论的贡献，她甚至从来没有机会理解。从 1990 年 5 月 1 日开始，她利用我离开电脑的时间把纸上的数据传输到电脑上。她因为打字很慢花了很多天，但这是值得的。

## Model of intermittent learning

## 间歇学习模型

Throughout the summer of 1990, instead of focusing on my [Master's Thesis](https://supermemo.guru/wiki/Optimization_of_learning), I worked on the "*model of intermittent learning*". It was not unusual for me to work for 10 hours straight, or go to sleep at 7 am empty-handed, or leave the computer churning the numbers overnight.

整个 1990 年的夏天，我没有专注于我的硕士论文，而是致力于 “间歇学习模式” 。对我来说，连续工作 10 个小时，早上 7 点空手而睡，或者让电脑整夜翻页，这些都很正常。

Persistence and tinkering pay. Only teens can afford it and should be given the space and the freedom. Despite being 28 years old, I was being tolerated at home pretty well. Like an immature teen. I lived at my sister's apartment where I could leech on her kindness. Long hours at the computer were excused as "*working on my Master's Thesis*". The truth was nobody asked me to do it, nobody demanded it, it did not even push [SuperMemo](https://supermemo.guru/wiki/SuperMemo) much ahead. It was a sheer case of scientific curiosity. I just wanted to know how memory works.

坚持和修补是有回报的。只有青少年才能负担得起，应该给他们空间和自由。尽管我已经28岁了，我在家里还挺能忍受的，像个不成熟的孩子。我住在姐姐的公寓里，我可以从她的善良中获益。长时间坐在电脑前可以被解释为“做我的硕士论文”。事实是，没有人要求我这么做，也没有人要求我这么做，它甚至没有提前推进 [SuperMemo](https://supermemo.guru/wiki/SuperMemo) 。这完全是出于对科学的好奇。我只是想知道记忆是如何运作的。

I had dozens of pages of questions and their [repetition history](https://supermemo.guru/wiki/Repetition_history). I tried to predict "memory lapses per page". I used [root-mean-square deviation](https://supermemo.guru/wiki/Deviation) for lapse prediction (denoted below as Dev). By Jul 10, 1990, Tuesday, I reached Dev<3 and felt like the problem was almost "solved." On Jul 12, 1990, I improved to Dev=2.877 (incidentally, my Thesis speaks of 2.887241). However, by Aug 27, 1990, I declared the problem unsolvable. My notes from that day say:

Personal anecdote. [Why use anecdotes?](https://supermemo.guru/wiki/Why_use_anecdotes%3F)

Aug 27, 1990: **I solved the problem** of intermittent learning **showing that it is unsolvable**! One parameter is not able to describe the strength of memory related to the whole page of items. This shows that **there are no optimal intervals for items with low E-factors**!

On Aug 30 1990, I decribed the model for my Master's Thesis. The text covered 15 pages that don't make for a good reading. I bet nobody has ever had the patience to read this all. That chapter was not even published at [supermemo.com](http://supermemo.com/) when my Master's Thesis was put on-line in excerpts in the late 1990s.

我有几十页的问题和他们的[重复历史](https://supermemo.guru/wiki/Repetition_history)。我试图预测“每一页的记忆缺失”。我使用 [root-mean-square deviation](https://supermemo.guru/wiki/Deviation) 来进行误差预测(如下图所示为Dev)。到 1990 年 7 月 10 日，星期二，我到达了 Dev < 3 ，并且感觉这个问题几乎已经“解决”了。 1990 年7 月 12 日，我提高到 Dev = 2.877 (顺便说一句，我的论文提到了 2.887241 )。然而，到 1990 年 8 月 27 日，我宣布这个问题无法解决。我那天的笔记说:

个人轶事。(为什么使用轶事?)

1990 年 8 月 27 日:**我解决了**间歇性学习的问题**，表明它是不可解决的**!一个参数无法描述与整个项目页相关的内存强度。这说明**对于低e因子的项目**没有最优的时间间隔!

1990 年 8 月 30 日，我描述了我硕士论文的模型。这篇课文有 15 页，不好读。我敢打赌没有人有耐心读完这篇文章。这一章甚至没有在 [supermemo.com](http://supermemo.com/) 上发表，我的硕士论文在上世纪 90 年代末被摘录放到网上。

However, the conclusions drawn on the basis of the model had a profound effect on my thinking about memory in the decades that followed. The whole idea behind the model is actually reminiscent of the optimizations used to deliver [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) (2014-2016).

When I declared the problem unsolvable, I meant that I could not accurately describe the memory of "difficult pages" as heterogenous materials require more complex models. However, Aug 31, 1990 notes sound far more optimistic:

Personal anecdote. [Why use anecdotes?](https://supermemo.guru/wiki/Why_use_anecdotes%3F)

Aug 31, 1990: :[...]*my formulas work only when intervals are not much shorter than the previous strength*

然而，在这个模型的基础上得出的结论对我在接下来的几十年里对记忆的思考产生了深远的影响。这个模型背后的整个思想实际上让人想起了用来交付 Algorithm SM-17 的优化。

当我宣布这个问题无法解决时，我的意思是我无法准确地描述“困难页面”的记忆，因为异质材料需要更复杂的模型。然而，1990年8月31日的报告听起来要乐观得多:

个人轶事。(为什么使用轶事?)

1990年8月31日:……我的公式只有在间隔不比以前的强度短多少的情况下才有效。

## Past (1990) vs. Present (2018)

## 过去(1990)与现在(2018)

Conclusions at the end of the chapter, and the procedure itself are reminiscent of the methodology I used in 2005 when looking for the universal formula for [memory stability](https://supermemo.guru/wiki/Stability) increase, and in 2014, when [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) was based on a far more accurate mathematical description of memory. Like the newest [SuperMemo](https://supermemo.guru/wiki/SuperMemo) algorithm, the model made it possible to compute [retention](https://supermemo.guru/wiki/Retention) for any repetition schedule. Naturally, it was far less accurate as it was based on inferior data. Moreover, what [SuperMemo 17](http://super-memo.com/supermemo17.html) does in real time, it took many hours of PC computer time back in 1990.

This old seemingly boring portion of my Master's Thesis has then grown in importance by now. I dare say that only inferior data separated that work from [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) that emerged long 25 years later. I quote the text with minor notational and stylistic improvements without the chapter about [forgetting curves](https://supermemo.guru/wiki/Forgetting_curve) that was erroneous due to highly heterogeneous material used in computations:

最后一章的结论，和过程本身让我想起，在 2005 年我正在寻找通用公式(内存稳定性)的增加的方法，在 2014 年算法 SM-17 基于一个更精确描述记忆的数学工具。就像最新的  SuperMemo 算法一样，该模型可以为任何重复计划计算[retention](https://supermemo.guru/wiki/Retention)。当然，它的准确性要差得多，因为它是基于劣质数据的。更重要的是，在 1990 年的时候，PC 机的工作时间要比现在长好几个小时。

我硕士论文中这个看起来很无聊的部分现在变得越来越重要了。我敢说，只有劣等数据才能将其与 25 年后出现的[Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17) 区分开来。我引用了文本，在注释和风格上做了细微的改进，但没有提到关于遗忘曲线的章节，这是由于计算中使用的材料高度异构而导致的错误:

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

存档警告:为什么使用 literal archives?

这篇文章是Piotr Wozniak的一部分：“改进学习” (1990)

**Model of intermittent learning**

**互动学习的模型**

The [SuperMemo](https://supermemo.guru/wiki/SuperMemo) model provides a basis for the calculation of [optimal intervals](https://supermemo.guru/wiki/Optimal_interval) that should separate repetitions in the process of time optimal learning.

However, it does not allow to predict the changes of memory variables if repetitions are done in irregular intervals.

Below I present an attempt to augment the SuperMemo model so that it can be used in the description of the process of intermittent learning.

[SuperMemo](https://supermemo.guru/wiki/SuperMemo)模型为计算[最优间隔](https://supermemo.guru/wiki/Optimal_interval)提供了一个基础，它应该在时间最优学习过程中分离重复。

但是，如果重复以不规律的间隔进行，则不允许预测记忆变量的变化。

下面，我提出了一个尝试，以扩大SuperMemo模型，使它可以用于描述间歇学习的过程。

In [Chapter 3](http://super-memory.com/english/ol/beginning.htm), I mentioned the way, in which I had learned English and biology before the [Algorithm SM-0](https://supermemo.guru/wiki/SuperMemo_on_paper) was developed.

Data collected during that time (1982-1984) provide an excellent basis for the construction of the model of intermittent learning. [Items](https://supermemo.guru/wiki/Item), formulated in compliance with the [minimum information principle](https://supermemo.guru/wiki/Minimum_information_principle) (usually having the form of pairs of words) were grouped in pages subject to the irregular review process.

The collected data, available in the computer readable form, include the description of repetitions of 71 pages, and in addition, 80 similar pages participating in a process supervised by the  

在[第3章](http://super-memory.com/english/ol/beginning.htm)中，我提到了在[算法SM-0](https://supermemo.guru/wiki/SuperMemo_on_paper)开发之前，我学习英语和生物学的方法。

在此期间(1982-1984年)收集的数据为间歇学习模型的建立提供了良好的基础。[Items](https://supermemo.guru/wiki/Item)按照[minimum information principle](https://supermemo.guru/wiki/Minimum_information_principle)(通常是成对的单词形式)进行分组，按照不规则的审查过程进行分组。

收集的数据以计算机可读的形式提供，其中包括71页的重复描述，另外还有80页类似的重复描述，这些重复描述参与了由

## Similarity to Algorithm SM-17

## 与算法SM-17相似

Note that the formulation of the problem is reminiscent of the procedure used to compute the [stability increase matrix (SInc[\])](https://supermemo.guru/wiki/Stability_increase) in [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17). [Memory stability](https://supermemo.guru/wiki/Stability) was rescaled to make it possible to interpret it as an [interval](https://supermemo.guru/wiki/Interval). Even the symbols are similar: S for [stability](https://supermemo.guru/wiki/Stability) and D for [deviation](https://supermemo.guru/wiki/Deviation). Page lapses substituted for [retrievability](https://supermemo.guru/wiki/Retrievability).

注意事项的制定是对过去计算[稳定增强矩阵(SInc[\]]]的程序的一种反映(https://supermemo.guru/wiki/稳定_增量)在[算法hm SM-17](https://supermemo.guru/wiki/算法hm_sm -17)。[记忆稳定](https://supermemo.guru/wiki/Stability)被要求使其有可能解释为一种[区间](https://supermemo.guru/wiki/区间)。甚至符号也很相似:S for [stability](https://supermemo.guru/wiki/Stability)和D for [deviation](https://supermemo.guru/wiki/Deviation)。Page lapses被替换为[retrievability](https://supermemo.guru/wiki/Retrievability)。

I loved playing with various optimization algorithms. You can still visually observe in [SuperMemo 17](http://super-memo.com/supermemo17.html) how the algorithm runs surface fitting optimizations (see [picture](https://supermemo.guru/wiki/Stability_increase)). Doing it with 12 variables might have been a bit inefficient, but I never cared about the method as long as I got interesting results that provided new insights into how memory works.

我喜欢玩各种优化算法。你仍然可以在[SuperMemo 17](http://super-memo.com/supermemo17.html)中直观地观察算法如何运行曲面拟合优化(参见[图片](https://supermemo.guru/wiki/Stability_increase))。用12个变量来做可能有点低效，但我从不关心这个方法，只要我得到了有趣的结果，为记忆的工作原理提供了新的见解。

For those familiar with [Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17), we changed the notation in the text below. In addition, we changed symbols such as In and Ln that in print could easily be misread as logarithms.

The list of changes:

Ln -> LapsnIn -> IntnDn -> DevnR -> RepNo

对于那些熟悉[Algorithm SM-17](https://supermemo.guru/wiki/Algorithm_SM-17)的人，我们修改了下面文本中的符号。此外，我们还更改了In和Ln等符号，这些符号在打印时很容易被误解为对数。

更改清单:

Ln -> LapsnIn -> IntnDn -> DevnR -

## Formulation of the problem of intermittent learning

## 间歇性学习问题的提法

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

档案警告:为什么使用literal archives?

这篇文章是[Piotr Wozniak]的一部分:“改进学习”(https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

**11.1. Formulation of the problem of intermittent learning**

**11.1.提出了间歇学习的问题**

1. There are 161 pages.
2. Each page contains about 40 items.
3. For each page, the description of the learning process (collected during experimental repetitions) has the following form:
4. 
5. Find the functions f and g described by the formulas:

1.全书161页。

2.每页大约包含40个项目。

3.每一页对学习过程的描述(实验重复时收集)如下:

4.

5.求公式所描述的函数f和g:

Note that functions f and g will provide a basis for valuable biological considerations only if they are simple and defined by a limited number of parameters (e.g. a*ln()+b or a*exp()+b etc.). Otherwise, one could always construct a gigantic, meaningless formula to automatically put Dev to zero.

注意，函数f和函数g只有在简单且由有限数量的参数(例如a*ln()+b或a*exp()+b等)定义的情况下，才能为有价值的生物学考虑提供基础。否则，人们总是可以构建一个庞大的、毫无意义的公式来自动将Dev置零。

## Solution to the problem of intermittent learning

## 解决间歇学习的问题

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

档案警告:为什么使用literal archives?

这篇文章是[Piotr Wozniak]的一部分:“改进学习”(https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

**11.2. Solution to the problem of intermittent learning**

**11.2.解决间歇学习的问题**

In the search for functions f and g that minimize the value of *Dev*, I used a numerical minimization procedure described in [Wozniak, 1988b](https://supermemo.guru/wiki/Optimization_of_learning_(1990):_References) (*A new algorithm for finding local maxima of a function within the feasible region. Credit paper*).

在寻找函数f和函数g使*Dev*的值最小化的过程中，我使用了[Wozniak, 1988b](https://supermemo.guru/wiki/Optimization_of_learning_(1990):_References)中描述的数值最小化过程(*在可行区域内查找函数的局部最大值的新算法)。信用论文)。

Exemplary functions used in the search could look as follows:

搜索中使用的示例函数如下:

> S(1)=x[1]
>
> S(n)=x[2]*Intn*exp(-Lapsn*x[3])+x[4])
>
> Laps(n)=x[5]*(1-exp(-Intn/S(n-1)))
>
>
> where:
>
> - x[i] - variables that are computed by the minimization procedure,
> - S(n), Laps(n), Lapsn and Intn - as defined in 11.1.
>
> S(1)=x[1]
>
> S(n)=x[2]*Intn*exp(-Lapsn*x[3])+x[4])
>
> Laps(n)=x[5]*(1-exp(-Intn/S(n-1)))
>
> 上式中：
>
> - x[i] -通过最小化程序计算的变量，
>
> - S(n)， Laps(n)， Lapsn和Intn -定义在11.1中。
>

Note, that the function f describing S(n) does not use S(n-1) as its argument (the formulation of the problem allows, but does not require, that the new strength be calculated on the base of the previous strength).

In order to retain simplicity and save time, I set a limit of 12 variables used in the process of minimization.

I tested a great gamut of mathematical functions constructed in accordance with obvious intuitions concerning memory (e.g. that with time passing by, the number of lapses of memory will increase).

These included exponential, logarithmic, power, hyperbolic, sigmoidal, bell-shaped, polynomial and reasonable combinations thereof.

注意，描述S(n)的函数没有使用S(n-1)作为它的参数(问题的提法允许，但不要求在以前的强度的基础上计算新的强度)。

为了保持简单性和节省时间，我设置了最小化过程中使用的12个变量的限制。

我测试了大量的数学函数，这些函数都是根据明显的记忆直觉构建的(例如，随着时间的流逝，记忆缺失的次数会增加)。

其中包括指数、对数、幂、双曲、s形、钟形、多项式及其合理组合。

In most cases, the minimization procedure reduced the value of Dev to less than 3, and functions f and g assumed similar shape independent of their nature.

The lowest value of Dev obtained with the use of fewer than 12 variables was 2.887241.

在大多数情况下，最小化过程将Dev的值降低到3以下，并且函数f和g的形状与它们的性质无关。



使用小于12个变量得到的Dev值最低为2.887241。

The functions f and g were as follows:

函数f和g如下:

```
constant S(1)=0.2104031;

function Sn(Intn,Lapsn,S(n-1));
begin
    S(n):=0.4584914*(Intn+1.47)*exp(-0.1549229*Lapsn-0.5854939)+0.35;
    if Lapsn=0 then
        if S(n-1)>In then
            S(n):=S(n-1)*0.724994
        else 
            S(n):=Intn*1.1428571;
end;

function Lapsn(Intn,S(n-1));
var quot;
begin
    quot:=(Intn-0.16)/(S(n-1)-0.02)+1.652668;
    Lapsn:=-0.0005408*quot*quot+0.2196902*quot+0.311335;
end;
```

Without significantly changing the value of Dev, these functions can be easily converted to the following form:

在不显著改变Dev值的情况下，这些函数可以很容易地转换为以下形式:

> S(1)=1
>
> for Intn>S(n-1): S(n)=1.5*Intn*exp(-0.15*Lapsn)+1
>
> Laps(n)=Intn/S(n-1)

Note that:

- particular elements of the function where dropped or rounded whenever the operation did not considerably affect the value of Dev,
- strength was rescaled to allow it to be interpreted as an interval for which the number of [lapses](https://supermemo.guru/wiki/Lapse) equals 1 and the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) equals 2.5% (there are 40 items on a page and 1/40=2.5%),
- the formula for strength can only be valid if Intn is not much less than S(n-1). This is because of the fact that the value S(n-1) must be used in calculation of S(n) if the number of [lapses](https://supermemo.guru/wiki/Lapse) is low, e.g. for Intn<=S(n-1): S(n)=S(n-1)*(1+0.5/(1-exp(S(n-1))*(1-exp(-Intn)))



- the formulas cannot be used to describe any process in which intervals are manifold longer than the optimal ones. This is because of the fact that for Intn->∞ the value of Laps(n) exceeds 100%,
- the formulas describe learning of collective items characterized by more or less uniform distribution of [E-Factors](https://supermemo.guru/wiki/E-Factor). Therefore it cannot be used universally for items of variable difficulty.

注意：

- -当操作没有显著影响Dev值时，删除或四舍五入的函数的特定元素，
- 强度被重新调整，允许它被解释为一个区间，其中[失误]的数量(https://supermemo.guru/wiki/失效)等于1，而[遗忘指数](https://supermemo.guru/wiki/Forgetting_index)等于2.5%(页面上有40个项目，1/40=2.5%)，
- 强度公式只有在Intn不小于S(n-1)时才有效。这是因为当[lapses](https://supermemo.guru/wiki/Lapse)的数目较低时，必须使用S(n-1)来计算S(n)，例如:Intn<=S(n-1): S(n)=S(n-1)*(1+0.5/(1-exp(S(n-1))*(1-exp(-Intn)))
- 这些公式不能用来描述任何时间间隔比最佳时间间隔长许多倍的过程。这是因为对于Intn->∞，圈数(n)超过100%，
- 该公式描述了具有或多或少均匀分布的[e - factor]的集体项目学习(https://supermemo.guru/wiki/E-Factor)。因此，它不能普遍用于难度可变的项目。

As for now, the above formulas make up the best description of the process of intermittent learning, and will later be referred to as the model of intermittent learning (IL model for short)

目前，上述公式是对间歇学习过程的最佳描述，以后将其称为间歇学习模型(IL model)

## Simulations based on the model of intermittent learning

## 基于间歇学习模型的仿真

With the formula found above, I could run a whole series of simulation experiments that would help me answer many hypothetical scenarios on the behavior of memory in various circumstances. Those simulations shaped the progress of [SuperMemo](https://supermemo.guru/wiki/SuperMemo) for many years to follow. In particular, the trade-off between workload and [retention](https://supermemo.guru/wiki/Retention) played a major role in optimization of learning as of [SuperMemo 6](https://supermemo.guru/wiki/SuperMemo_6) (1991). Until this day, it is the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) (or [retrievability](https://supermemo.guru/wiki/Retrievability)) that provide the guiding criterion in learning, not the intuitively natural [increase in memory stability](https://supermemo.guru/wiki/Stability_increase) that may occur at lower levels of [recall](https://supermemo.guru/wiki/Recall). Set level of memory [lapses](https://supermemo.guru/wiki/Lapse) played the role of the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) below.

有了上面发现的公式，我就可以进行一系列的模拟实验，这将帮助我回答各种情况下关于记忆行为的许多假设场景。这些模拟塑造了[SuperMemo](https://supermemo.guru/wiki/SuperMemo)随后多年的发展。特别是，从[SuperMemo 6](https://supermemo.guru/wiki/Retention)(1991)开始，工作负载和[retention](https://supermemo.guru/wiki/SuperMemo_6)之间的平衡在优化学习中起了重要作用。直到今日,(忘记指数)(https://supermemo.guru/wiki/Forgetting_index)或(可恢复性)(https://supermemo.guru/wiki/Retrievability)提供学习的指导标准,而不是凭直觉自然(增加内存稳定性)(https://supermemo.guru/wiki/Stability_increase),可能发生在较低的水平(召回)(https://supermemo.guru/wiki/Recall)。设置记忆水平[lapses](https://supermemo.guru/wiki/Lapse)在下面扮演[遗忘索引](https://supermemo.guru/wiki/Forgetting_index)的角色。

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

档案警告:为什么使用literal archives?

这篇文章是[Piotr Wozniak]的一部分:“改进学习”(https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

**11.4. Verification of the model of intermittent learning**

**11.4.间歇学习模型的验证**

To verify the consistency of the model of intermittent learning with the SuperMemo theory, let us try to calculate optimal intervals that should separate repetitions.

为了验证间歇学习模型与SuperMemo理论的一致性，让我们尝试计算应该分离重复的最佳间隔。

The optimal interval will be determined by the moment at which the number of [lapses](https://supermemo.guru/wiki/Lapse) reaches a selected value Lapso.

最佳时间间隔将由[失误次数](https://supermemo.guru/wiki/失效)达到选定值Lapso的时刻决定。

The algorithm proceeds as follows:

算法流程如下:

1. i:=1

2. S(i):=1

3. Find Int(i+1) such that Laps(i+1) equals Laps

   o

   . Use the formula:

4. i:=i+1

5. S(i):=1.5*Int(i)*exp(-0.15*Lapso)+1 (taken from the IL model)

6. goto 3

If Lapso equals 2.5 ([forgetting index](https://supermemo.guru/wiki/Forgetting_index) 6.25%) and the exact variant of the model of intermittent learning is used then an amazing correspondence can be observed (compare the experiment presented on page 16, [Chapter 3.1](https://supermemo.guru/wiki/Birthday_of_SuperMemo)):

如果Lapso = 2.5((忘记指数)(https://supermemo.guru/wiki/Forgetting_index) 6.25%)和间歇的精确模型的变体使

- Rep - number of the repetition
- Interval - optimal interval preceding the repetition, determined by Lapso=2.5 on the base of the IL model,
- Factor - optimal factor equal to the quotient of the optimal interval and previously used optimal interval,
- SM-0 - optimal interval calculated on the base of experiments leading to the algorithm SM-0

用学习然后可以观察到一个了不起的书信(比较实验提出了16页,(3.1章)(https://supermemo.guru/wiki/Birthday_of_SuperMemo)):

- 重复次数
- Interval -重复之前的最佳间隔，由Lapso=2.5在IL模型的基础上确定，
- 因子-最优因子等于最优区间与以前使用的最优区间之商，
- SM-0 -在实验的基础上计算出最优区间，得出SM-0算法

| Rep  | Interval | Factor | SM-0 |
| :--: | :------: | :----: | :--: |
|  2   |   1.8    |        |  1   |
|  3   |   7.8    |  4.36  |  7   |
|  4   |   16.8   |  2.15  |  16  |
|  5   |   30.4   |  1.80  |  35  |
|  6   |   50.4   |  1.66  |      |
|  7   |   80.2   |  1.59  |      |
|  8   |   124    |  1.55  |      |
|  9   |   190    |  1.53  |      |
|  10  |   288    |  1.52  |      |
|  11  |   436    |  1.51  |      |
|  12  |   654    |  1.50  |      |
|  13  |   981    |  1.50  |      |
|  14  |   1462   |  1.49  |      |
|  15  |   2179   |  1.49  |      |
|  16  |   3247   |  1.49  |      |
|  17  |   4838   |  1.49  |      |
|  18  |   7209   |  1.49  |      |

Obviously, the exact correspondence, to some extent, is a coincidence because the experiment leading to the formulation of the algorithm SM-0 was not that sensitive.

It is worth noticing, that **optimal factors tend to decrease gradually!** This fact seems to confirm recent observations based on the analysis of the [matrix of optimal factors](https://supermemo.guru/wiki/OF_matrix) used in [Algorithm SM-5](https://supermemo.guru/wiki/Algorithm_SM-5).

显然，精确的对应在某种程度上是一种巧合，因为导致算法SM-0形成的实验并不那么敏感。

值得注意的是，**最优因子逐渐减少!**这一事实似乎证实了最近在[算法SM-5](https://supermemo.guru/wiki/OF_matrix)中使用的[矩阵的最优因素](https://supermemo.guru/wiki/Algorithm_SM-5)的分析。

If Lapso equals 4 ([forgetting index](https://supermemo.guru/wiki/Forgetting_index) 10%, as in [Algorithm SM-5](https://supermemo.guru/wiki/Algorithm_SM-5)) then the sequence of [optimal factors](https://supermemo.guru/wiki/Optimal_factor) resembles a column of the [OF matrix](https://supermemo.guru/wiki/OF_matrix) in [Algorithm SM-5](https://supermemo.guru/wiki/Algorithm_SM-5). Also the knowledge retention matches almost ideally the one found in SM-5 [databases](https://supermemo.guru/wiki/Database).

If Lapso equals 4 ([forgetting index] (https://supermemo.guru/wiki/Forgetting_index) 10%, sda[算法SM-5] (https://supermemo.guru/wiki/Algorithm_SM-5) then the董事会of optimal因素](https://supermemo.guru/wiki/Optimal_factor) resembles栏of the [] (https://supermemo.guru/wiki/OF_matrix母体)[算法SM-5] (https://supermemo.guru/wiki/Algorithm_SM-5)。在SM-5 (https://supermemo.guru/wiki/Database)中发现的知识保留匹配。	

| Rep  | Interval | [Retention](https://supermemo.guru/wiki/Retention) | Factor |
| :--: | :------: | :------------------------------------------------: | :----: |
|  2   |    3     |                      93.21678                      |        |
|  3   |    16    |                      93.80946                      |  4.89  |
|  4   |    43    |                      93.97184                      |  2.74  |
|  5   |   102    |                      94.04083                      |  2.39  |
|  6   |   232    |                      94.06886                      |  2.27  |
|  7   |   517    |                      94.08418                      |  2.23  |
|  8   |   1138   |                      94.09256                      |  2.20  |
|  9   |   2502   |                      94.09737                      |  2.20  |
|  10  |   5481   |                      94.09967                      |  2.19  |

The value of [retention](https://supermemo.guru/wiki/Retention) was obtained by averaging its value calculated for each day of the optimal process:

[retention](https://supermemo.guru/wiki/Retention)的值是通过计算最优过程中每天的平均值得到的:

> R=(R(1)+R(2)+...+R(n))/n
>
> R(d)=100-2.5*Laps(d-dlr)
>
>
> where:
>
> - R - average retention
> - R(d) - retention on the d-th day of the process
> - Laps(Int) - expected number of lapses after the interval I
> - dlr - day of the process on which the last repetition was scheduled
>
> R=(R(1)+R(2)+...+R(n))/n
>
> R(d)=100-2.5*Laps(d-dlr)
>
> 上式中：
>
> - R -平均保留率
> - R(d) -保留第d天的过程
> - 圈数(Int) -间隔I后的期望圈数
>
> - dlr -最后一次重复预定日期的流程日期

## Workload vs. Retention trade-off

## 工作量与保留的权衡

Despite the inaccuracies coming from the heterogeneous material, solid conclusions could be drawn about the impact of the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) on the amount of time needed to invest in learning. Those observations survived the test of time:

尽管来自异质材料的不准确之处，关于[遗忘指数](https://supermemo.guru/wiki/Forgetting_index)对学习所需时间的影响可以得出可靠的结论。这些观察经受住了时间的考验:

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

档案警告:为什么使用literal archives?

这篇文章是[Piotr Wozniak]的一部分:“改进学习”(https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

Very interesting conclusions may be drawn by comparison of retention and workload data calculated by means of the model of intermittent learning:

- [Index](https://supermemo.guru/wiki/Forgetting_index) - forgetting index (Lapso*2.5) determining optimal intervals in the process of time optimal learning scheduled with the use of the IL model
- [Retention](https://supermemo.guru/wiki/Retention) - overall retention obtained while using the given [forgetting index](https://supermemo.guru/wiki/Forgetting_index) (calculated upon elapse of 10,000 days)
- Repetitions - number of repetitions scheduled in the first 10,000 days of the process when using the given [forgetting index](https://supermemo.guru/wiki/Forgetting_index),
- [Factor](https://supermemo.guru/wiki/Optimal_factor) - asymptotic value of the optimal factor (taken from the 10,000-th day of the process)

通过对通过间歇学习模型计算出的保留率和工作量数据进行比较，可以得出非常有趣的结论:

- [Index](https://supermemo.guru/wiki/Forgetting_index) -遗忘指数(Lapso*2.5)确定使用IL模型调度的时间过程中最优学习的最优间隔
- [Retention](https://supermemo.guru/wiki/Retention) -使用给定的[遗忘指数]获得的总体保留率(https://supermemo.guru/wiki/Forgetting_index)(以过去10,000天计算)

- 重复次数-使用给定的[遗忘指数](https://supermemo.guru/wiki/Forgetting_index)时，在过程的前10,000天中计划的重复次数，

- [Factor](https://supermemo.guru/wiki/Optimal_factor) -最优因子的渐近值(取自过程的第10,000天)

| [Index](https://supermemo.guru/wiki/Forgetting_index) | [Retention](https://supermemo.guru/wiki/Retention) | Repetitions  | [Factor](https://supermemo.guru/wiki/Optimum_factor) |
| :---------------------------------------------------: | :------------------------------------------------: | :----------: | :--------------------------------------------------: |
|                          2.5                          |                       97.76                        | every 2 days |                        1.0000                        |
|                          4.5                          |                       96.88                        |      65      |                        1.0300                        |
|                          5.0                          |                       96.64                        |      30      |                        1.1600                        |
|                          5.5                          |                       96.39                        |      22      |                        1.3000                        |
|                         6.25                          |                       96.01                        |      17      |                        1.4900                        |
|                          7.5                          |                       95.37                        |      13      |                        1.7700                        |
|                         10.0                          |                       94.10                        |      10      |                        2.1900                        |
|                         12.5                          |                       92.78                        |      9       |                        2.4700                        |

Figure 11.2 demonstrates that the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) used in determination of optimal intervals should fall into the range 5 to 10%.

图11.2显示了用于确定最佳时间间隔的[forget index](https://supermemo.guru/wiki/Forgetting_index)应该在5到10%之间。

[![Workload-retention trade-off](https://supermemo.guru/images/thumb/1/1e/Workload-retention_tradeoff.jpg/548px-Workload-retention_tradeoff.jpg)](https://supermemo.guru/wiki/File:Workload-retention_tradeoff.jpg)

> ***Fig. 11.2. Workload-retention trade-off:*** *On one hand, if forgetting index is lower than 5%, then the workload increases dramatically without substantially affecting the retention. On the other, above forgetting index of 10%, workload hardly changes while retention steadily falls down. Obviously, the workload-retention trade-off corresponds directly to the compromise between the acquisition rate and retention. By increasing the availability of time X times (by decreasing the workload X times), one can increase the acquisition rate X times (compare Chapter 5). Note, that the relation of the forgetting index and retention in this model is almost linear.* (source: [Optimization of learning](https://supermemo.guru/wiki/Optimization_of_learning): *Model of intermittent learning*, [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak), 1990)
>
> ***图. 11.2. Workload-retention（保留工作量)权衡:***一方面，如果遗忘指数低于5%，那么工作负荷会显著增加，而不会对记忆产生实质性的影响。另一方面，超过10%的遗忘指数，工作负荷几乎没有变化，而保留率则稳步下降。显然，工作量保留取舍直接对应于获取率和保留率之间的折衷。通过增加时间X倍的可用性(通过减少工作量X倍)，可以增加习得率X倍(比较第5章)。注意，该模型中遗忘指数与保留度几乎是线性关系。*(来源:[优化学习](https://supermemo.guru/wiki/Optimization_of_learning): *间歇学习模式*，[Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak)， 1990)*

Another important observation comes from the calculation of the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) for which the increase of strength is the greatest.

另一个重要的观察来自于对[遗忘指数](https://supermemo.guru/wiki/Forgetting_index)的计算，该指数的强度增长是最大的。

From the model of intermittent learning it follows that

从间歇学习的模型可以得出

> S(n)=1.5*Laps(n)*S(n-1)*exp(-0.15*Laps(n))+1

Upon differentiation for the variable Laps(n) we arrive at:

对变量Laps(n)进行微分，得到:

> S'(n)=1.5*S(n-1)*exp(-0.15*Laps(n))*(1-0.15*Laps(n))

Finally, after equating with zero, we obtain:

最后，代入0，得到:

> Laps(n)=7.8

which corresponds to the   ). However, it must not be forgotten that it is the knowledge retention and not the strength of memory that is the only factor traded for workload. Therefore the above finding does not abolish the validity of[Algorithm SM-5](https://supermemo.guru/wiki/Algorithm_SM-5)

对应于)。然而，不能忘记的是，知识的保留而不是记忆的强度是用来交换工作量的唯一因素。因此，上述发现并没有消除[算法SM-5](https://supermemo.guru/wiki/Algorithm_SM-5)的有效性

## Conclusions: model of intermittent learning

## 结论:间歇学习模型

The ultimate conclusions drawn at the end of the chapter stood the test of 3 decades. Only the claim on non-exponential shape of [forgetting curves](https://supermemo.guru/wiki/Forgetting_curve) is inaccurate. As the entire model was based on heterogeneous data, the [exponential nature of forgetting](https://supermemo.guru/wiki/Exponential_nature_of_forgetting) could not have been revealed.

本章最后得出的最终结论经受了30年的考验。只有关于[遗忘曲线](https://supermemo.guru/wiki/Forgetting_curve)的非指数形状的说法是不准确的。由于整个模型基于异构数据，[遗忘的指数性质](https://supermemo.guru/wiki/exponential_nature_of_forget)不可能被揭示。

Archive warning: [Why use literal archives?](https://supermemo.guru/wiki/Why_use_literal_archives%3F)

This text was part of: "*Optimization of learning*" by [Piotr Wozniak](https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

档案警告:为什么使用literal archives?

这篇文章是[Piotr Wozniak]的一部分:“改进学习”(https://supermemo.guru/wiki/Piotr_Wozniak) (1990)

**Interim summary**

- The model of intermittent learning was constructed making it possible to estimate knowledge retention upon different repetition schedules
- The model strongly indicates that the [forgetting curve](https://supermemo.guru/wiki/Forgetting_curve) is not exponential [comment 2018: wrong conclusion: compare [Exponential nature of forgetting](https://supermemo.guru/wiki/Exponential_nature_of_forgetting)]
- The model satisfactorily corresponds to experimental data
- With a striking accuracy, the model approximates optimal intervals and knowledge retention implied by the SuperMemo model
- The model indicates that optimal factors decrease in successive repetitions and asymptotically approach the ultimate value
- The model indicates that the desirable value of the [forgetting index](https://supermemo.guru/wiki/Forgetting_index) used in time optimal learning should fall into the range 5% to 10%
- The model indicates an almost linear relation of the forgetting index and knowledge retention
- The model shows that the greatest increase of the strength of memory occurs when intervals are approximately 2 times longer than that used in the [SuperMemo method](https://supermemo.guru/wiki/SuperMemo). This is equivalent to the forgetting index equal to 20%

**临时摘要**

- 建立了间歇学习模型，从而能够估计不同重复学习计划下的知识保留情况

- 模型强烈表明[遗忘曲线](https://supermemo.guru/wiki/Forgetting_curve)不是指数型[点评2018:错误结论:比较[遗忘的指数性质](https://supermemo.guru/wiki/exponential_nature_of_forget)]
- 模型与实验数据吻合较好
- 以惊人的准确性，该模型接近SuperMemo模型所暗示的最佳时间间隔和知识保留
- 该模型表明，最优因子在逐次重复中递减，渐近于极值
- 模型表明，用于时间最优学习的[遗忘指数](https://supermemo.guru/wiki/Forgetting_index)的可取值应在5% - 10%之间
- 该模型表明遗忘指数与知识保留之间几乎是线性关系
- 该模型表明，当间隔时间比[SuperMemo方法](https://supermemo.guru/wiki/SuperMemo)中使用的间隔时间大约长2倍时，内存强度的增加最大。这相当于遗忘指数等于20%